<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <!--<base target="_blank">-->
  <title>Diganta Misra</title>
  
  <meta name="author" content="Diganta Misra">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/favicon.png">
  <script src="scramble.js"></script>
  <!-- Place this tag in your head or just before your close body tag. -->
  <script async defer src="https://buttons.github.io/buttons.js"></script>
  <script src="hidebib.js" type="text/javascript"></script>
  <script type="text/x-mathjax-config">
			MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
		</script>
		<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
		</script>
</head>

<body>
  <table style="width:100%;max-width:920px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tbody><tr><td>
          <p style="text-align:center">
              <img src="images/nav.gif" alt="nav_sign" width="400" height="120">
            <br>
            <br>
            <email>
                <font id="email" style="display:inline;">kai@asdneat.ldpiaang</font>
                  <script>
                emailScramble = new scrambledString(document.getElementById('email'),
                    'emailScramble', 'kai@asdneat.ldpiaang',
                    [14, 4, 2, 8, 15, 13, 1, 5, 17, 7, 6, 18, 9, 12, 16, 20, 10, 19, 11, 3]);
              </script> </email>
            
            </p>
		
		<!--<table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">-->
		<table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
			<tr style="padding:0px">
				<td style="padding:0px">

				<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
					<tr style="padding:0px">
					<td style="padding:1.5%;width:16%;vertical-align:middle;text-align:center">
						<a href="index.html" target="_self">Home</a>
					</td>
					<td style="padding:1.5%;width:16%;vertical-align:middle;text-align:center">
						<a href="#news" target="_self">News</a>
					</td>
					<td style="padding:1.5%;width:16%;vertical-align:middle;text-align:center">
						<a href="#experience" target="_self">Experience</a>
					</td>
					<td style="padding:1.5%;width:16%;vertical-align:middle;text-align:center">
						<a href="#research" target="_self">Research</a>
					</td>
					<td style="padding:1.5%;width:16%;vertical-align:middle;text-align:center">
						<a href="#education" target="_self">Education</a>
					</td>
					<td style="padding:1.5%;width:16%;vertical-align:middle;text-align:center">
						<a href="#achievements" target="_self">Achievements</a>
					</td>          
					</tr>
				</table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:top">
			  <p>
				  I am a <i style="color:red;">Research MSc student</i> at <a href="https://mila.quebec/" target="_blank">MILA</a> affiliated with <a href="https://diro.umontreal.ca/accueil/" target="_blank">University of Montr√©al</a> supervised by <a href="https://sites.google.com/site/irinarish/" target="_blank">Associate Professor Irina Rish</a>. <img src="images/can.png" width="18px">
			  </p>
			  <p>
				  I also am the <i>Founder</i> and <i>President</i> at <a href="http://landskape.ai" target="_blank">Landskape AI</a>, a theoretical and analytical deep learning non-profit research organization. Previously, I served as a Machine Learning Engineer at <a href="https://wandb.ai/site" target="_blank">Weights & Biases</a> where I worked on the Frameworks and Integration Team. 
			  </p>
              <p>
                I broadly work on theoretical and analytical deep learning with focus on but not limited to the following domains:
				<ul>
					<li>Model Reprogramming</li> 
					<li>Sparsity</li> 
					<li>Adversarial Robustness</li>
					<li>Continual Learning</li>
				</ul>
              </p>
              <p>
                Presently, I am working as a Visiting Research Scholar on topics of <i>Sparsity</i> at <a href="https://vita-group.github.io/index.html" target="_blank">VITA, UT-Austin</a>, under <a href="https://spark.adobe.com/page/CAdrFMJ9QeI2y/" target="_blank">Dr. Zhangyang Wang</a>. 
				<br>
				In the past I have been fortunate to work with the likes of <a href="https://sites.google.com/site/dramritachaturvedi/home" target="_blank">Dr. Amrita Chaturvedi</a> from Indian Institute of Technology, Varanasi (IIT-BHU) in the field of biomedical data analysis and <a href="https://scholar.google.com/citations?user=iK_wjZsAAAAJ&hl=en" target="_blank">Vijay Kumar Verma</a> from <a href="https://www.isro.gov.in/" target="_blank">Indian Space Research Organization (ISRO)</a> in the domain of Genetic Algorithms.
              </p>
              <p style="padding:4.5%;text-align:center">
                <a href="data/CV.pdf" target="_blank">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=LwiJwNYAAAAJ&hl=en" target="_blank">Google Scholar</a> &nbsp/&nbsp
				<a href="https://github.com/digantamisra98" target="_blank">GitHub</a> &nbsp/&nbsp
				<a href="https://blog.paperspace.com/author/diganta/" target="_blank">Blog</a> &nbsp/&nbsp
				<a href="https://twitter.com/__z__9" target="_blank">Twitter</a>
				<br>
				<br>
				<a class="github-button" href="https://github.com/sponsors/digantamisra98" data-color-scheme="no-preference: light; light: light; dark: light;" data-icon="octicon-heart" data-size="large" aria-label="Sponsor @digantamisra98 on GitHub">Sponsor</a>
			  </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/new1.png"><img style="width:100%;max-width:100%; vertical-align:top" alt="profile photo" src="images/new1.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
			<a id="news"><h2>News</h2></a>
			<p>
            <div style="width:100%; height:230px;">
                <ul id="news">
					<li>November 2022: Our work on <a href="app2022" target="_blank">APP: Anytime Progressive Pruning</a> is accepted to the <a href="https://slowdnn-workshop.github.io/index.html" target="_blank">SlowDNN</a> workshop, 2023</a>.</li>
					<li>November 2022: Our work on <a href="app2022" target="_blank">APP: Anytime Progressive Pruning</a> is accepted to the <a href="https://continual-lifelong-learners.github.io/" target="_blank">Continual Lifelong Learning (CLL)</a> workshop at <a href="https://www.acml-conf.org/2022/" target="_blank">ACML, 2022</a>.</li>
					<li>July 2022: Our work on <a href="app2022" target="_blank">APP: Anytime Progressive Pruning</a> is accepted to the <a href="https://www.sparseneural.net/" target="_blank">Sparsity in Neural Network (SNN)</a> workshop, 2022.</li>
					<li>June 2022: Our work on <a href="scole2022" target="_blank">Scaling the Number of Tasks in Continual Learning</a> got accepted to the <a href="https://lifelong-ml.cc/" target="_blank">CoLLAs 2022 workshop</a>.</li>
					<li>June 2022: Our work on <a href="app2022" target="_blank">APP: Anytime Progressive Pruning</a> is accepted to the <a href="https://dynn-icml2022.github.io/" target="_blank">Dynamic Neural Network (DyNN) workshop</a> at <a href="https://icml.cc/Conferences/2022/" target="_blank">ICML, 2022</a>.</li>
					<li>May 2022: Awarded the MILA Entrepreneurs Grant worth CAD$5,000.</li>
					<li>May 2022: Awarded the <a href = "https://www.ai-week.ca/?utm_source=google-ads&utm_medium=cpc&utm_campaign=ai-week&utm_term=amii%20ai%20week&utm_campaign=AI-Week+%7C+S+%7C+Brand&utm_source=adwords&utm_medium=ppc&hsa_acc=6591753441&hsa_cam=16953749208&hsa_grp=135907011819&hsa_ad=593686735388&hsa_src=g&hsa_tgt=kwd-1650174358069&hsa_kw=amii%20ai%20week&hsa_mt=p&hsa_net=adwords&hsa_ver=3&gclid=CjwKCAjwve2TBhByEiwAaktM1BjAxiVdVUehV3fuuvfAgtH1vgzVT_jb-fmmTT6sbtfQSoxJ1RTJihoCLykQAvD_BwE" target = "_blank">AI Week 2022</a> Student Travel Bursary worth CAD$1,500.</li>
					
                </ul>
            </div>
			</p>
          </tr>
        </tbody></table>

		<br>
		<br>
		<br>
        
        <table width="100%" border="0" cellspacing="15" cellpadding="10">
          <a id="experience"><heading>&nbsp;&nbsp;&nbsp;Research Experience</heading></a>

		  <tr>
            <td width="15%" valign="center" align="center"><img src="https://yt3.ggpht.com/ytc/AKedOLQj45M52xLSICFBv-A0OBtuTZI5b688ty19ziMe=s900-c-k-c0x00ffffff-no-rj" alt="nthu" width="120" height="120"></td>
            <td width="85%" valign="top">
              <p>
                <span><strong>Researcher</strong></span><span style="float:right">April. 2022 - Present</span> <br>
                <em><a href="https://vita-group.github.io/index.html" target="_blank">Morgan Stanley</a></em>
                <br>
                Supervisor: <a href="https://scholar.google.de/citations?user=cfIrwmAAAAAJ&hl=en" target="_blank">Kashif Rasul</a>
                <br>
				Research Area: Continual Learning, Time Series, Model Reprogramming
				<br>
              </p>
            </td>
          </tr> 
		 
		  <tr>
            <td width="15%" valign="center" align="center"><img src="images/vita.png" alt="nthu" width="170" height="80"></td>
            <td width="85%" valign="top">
              <p>
                <span><strong>Visiting Research Scholar</strong></span><span style="float:right">Aug. 2021 - Present</span> <br>
                <em><a href="https://vita-group.github.io/index.html" target="_blank">VITA</a>, University of Texas at Austin</em>
                <br>
                Supervisor: <a href="https://spark.adobe.com/page/CAdrFMJ9QeI2y/" target="_blank"> Dr. Zhangyang Wang</a>
                <br>
				Research Area: Sparsity, Robustness and Knowledge Distillation.
				<br>
              </p>
            </td>
          </tr>

          <tr>
            <td width="15%" valign="center" align="center"><img src="images/hku.jpeg" alt="nthu" width="80" height="100"></td>
            <td width="85%" valign="top">
              <p>
                <span><strong><a href="https://www.lsr.hku.hk/member/diganta-misra/" target="_blank">Research Associate</a></strong></span><span style="float:right">Feb. 2020 - Present</span> <br>
                <em><a href="https://www.lsr.hku.hk/" target="_blank">Laboratory of Space Research (LSR)</a>, University of Hong Kong</em>
                <br>
                Supervisor: <a href="https://www.physics.hku.hk/people/academic/5206" target="_blank"> Dr. Quentin A. Parker</a>
                <br>
				Research Area: Computer Vision applications in PNe Exploration.
				<br>
              </p>
            </td>
          </tr>

          <tr>
            <td width="15%" valign="center" align="center"><img src="images/bu.png" alt="nthu" width="90" height="100"></td>
            <td width="85%" valign="top">
              <p>
                <span><strong>Research Intern</strong></span><span style="float:right">Jun. 2018 - Aug. 2018</span> <br>
                <em><a href="https://www.bennett.edu.in/laboratories/" target="_blank">NVIDIA AI Lab,</a> Bennett University</em>
                <br>
                Supervisors: <a href="https://www.gdeepak.com/" target="_blank">Dr. Deepak Garg</a> and <a href="https://sites.google.com/view/ksuneet/home" target="_blank">Dr. Suneet Gupta</a>
                <br>
				Research Area: Large Scale Visual Recognition.
				<br>
              </p>
            </td>
          </tr>
        </table>


		<table width="100%" border="0" cellspacing="15" cellpadding="10">
		<heading>&nbsp;&nbsp;&nbsp;Industrial and Leadership Experience</heading>

		<tr>
			<td width="15%" valign="center" align="center"><img src="images/lskape.png" alt="nthu" width="80" height="80"></td>
			<td width="85%" valign="top">
			<p>
				<span><strong><a href="https://landskape.ai/member/diganta/" target="_blank">Founder, President and Researcher</a></strong></span><span style="float:right">Sept. 2019 - Present</span> <br>
				<em><a href="https://landskape.ai/" target="_blank">Landskape AI</a></em>
				<br>
				Mentors: <a href="https://sites.google.com/site/jaegulchoo/" target="_blank"> Assc. Prof. Jaegul Choo</a>, <a href="https://ideami.com/ideami/" target="_blank">Javier Ideami</a> and <a href="https://twitter.com/federicolois" target="_blank">Federico Lois</a>
				<br>
				Research Area: Analytical Deep Learning Theory.
				<br>
			</p>
			</td>
		</tr>

		<tr>
			<td width="15%" valign="center" align="center"><img src="images/wandb.jpeg" alt="nthu" width="80" height="80"></td>
			<td width="85%" valign="top">
			<p>
				<span><strong>Machine Learning Engineer</strong></span><span style="float:right">Dec. 2020 - Oct. 2021</span> <br>
				<em><a href="https://wandb.ai/site" target="_blank">Weights & Biases</a></em>
				<br>
				Team: Frameworks and Integrations.
				<br>
			</p>
			</td>
		</tr>

		<tr>
			<td width="15%" valign="center" align="center"><img src="images/paperspace.png" alt="nthu" width="80" height="80"></td>
			<td width="85%" valign="top">
			<p>
				<span><strong>Technical Content Developer</strong></span><span style="float:right">Jun. 2020 - Jan. 2021</span> <br>
				<em><a href="https://www.paperspace.com/" target="_blank">Paperspace</a></em>
				<br>
				<a href="https://blog.paperspace.com/author/diganta/" target="_blank">Blog</a>
				<br>
				Topic Area: Computer Vision (Attention Mechanisms).
				<br>
			</p>
			</td>
		</tr>
		</table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <a id="research"><heading>Publication</heading></a>
              <br />*indicates equal contribution 
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

			<tr bgcolor="#ffffd0">
        	<td width="15%" valign="center" align="center"><img src="images/landskape-relu-mish-july-20-Fixed.jpg" alt="nthu" width="290" height="240"></td>
            </td>
            <td width="85%" style="padding:20px;vertical-align:middle">
                  <a class="tog" href="#mish">
                    <papertitle>Mish: A Self Regularized Non-Monotonic Neural Activation Function</papertitle>
                  </a>
                  <br>
                  <strong>Diganta Misra</strong>
                  <br>
                  <p></p>
                  <em>BMVC, 2020</em>
                  <div class="paper" id="mish2020">
                    <a class="tog" href="https://github.com/digantamisra98/Mish" target="_blank">project</a> /
                    <a href="https://www.bmvc2020-conference.com/assets/papers/0928.pdf" target="_blank">paper</a> /
                      <a class="tog" href="javascript:toggleblock('mish_abs')">abstract</a> / 
                      <a class="tog" href="javascript:toggleblock('mish_bib')">bibtex</a>
                      <p align="justify">
                        <i id="mish_abs">
                            We propose <b>Mish</b>, a novel self-regularized non-monotonic activation function which can be mathematically defined as: $f(x)=xtanh(softplus(x))$. As activation functions play a crucial role in the performance and training dynamics in neural networks, we validated experimentally on several well-known benchmarks against the best combinations of architectures and activation functions. We also observe that data augmentation techniques have a favorable effect on benchmarks like ImageNet-1k and MS-COCO across multiple architectures. For example, Mish outperformed Leaky ReLU on YOLOv4 with a CSP-DarkNet-53 backbone on average precision ($AP^{val}_{50}$) by $2.1\%$ in MS-COCO object detection and ReLU on ResNet-50 on ImageNet-1k in Top-1 accuracy by $\approx 1 \%$ while keeping all other network parameters and hyperparameters constant. Furthermore, we explore the mathematical formulation of Mish in relation with the Swish family of functions and propose an intuitive understanding on how the first derivative behavior may be acting as a regularizer helping the optimization of deep neural networks.
                        </i>
                      </p>
                      <bibtext xml:space="preserve" id="mish_bib">
						@article{misra2019mish, <br />
							title={Mish: A self regularized non-monotonic neural activation function}, <br />
							author={Misra, Diganta}, <br />
							journal={arXiv preprint arXiv:1908.08681}, <br />
							volume={4}, <br />
							pages={2}, <br />
							year={2019}, <br />
							publisher={CoRR} <br />
						  }
                            
                        </bibtext>
					<a href="https://youtu.be/whOdg-yrgdI" target="_blank">CV Talk Episode</a> /
					<a href="https://open.spotify.com/episode/4sT9sxjSbAKtvJ6hTFg9zc" target="_blank">ML Cafe Episode</a> /
					<a href="https://youtu.be/T2CRFROKcLM" target="_blank">Sicara Talk</a> /
					<a href="https://www.youtube.com/watch?v=1U-7TWysqIg" target="_blank">W&B Salon Episode</a>
					<br>
					<br>
					<img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/digantamisra98/mish?style=social">
					&emsp;
					<img alt="GitHub forks" src="https://img.shields.io/github/forks/digantamisra98/Mish?style=social">
					&emsp;
					<a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=LwiJwNYAAAAJ&citation_for_view=LwiJwNYAAAAJ:u5HHmVD_uO8C" target="_blank"><img src="https://img.shields.io/badge/Citations-1024-lightgrey.svg?style=social&logo=Google Scholar"></a>
					<br>
					<a href="https://wandb.ai/diganta/Mish?workspace=user-diganta" target="_blank"><img src="https://img.shields.io/badge/Dashboard-WandB?style=flat&color=black&logo=data:image/png;base64,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"></a>
                    &emsp;
					<a href="https://console.paperspace.com/github/digantamisra98/Mish/blob/master/exps/Mish_test.ipynb" target="_blank">
					<img src="https://assets.paperspace.io/img/gradient-badge.svg" alt="Run on Gradient"/>
					<br>
					<i style="color:rgb(225, 0, 255);">For those who are curious, the name</i> <i style="color:rgb(255, 0, 132);">Mish</i> <i style="color:rgb(225, 0, 255);">was coined by my girlfriend.</i> üë©‚Äçüíª
					</a>
					</div>              
                    <script language="JavaScript">hideblock('mish_bib');hideblock('mish_abs');</script>
            </td>
            </tr>

			<td width="15%" valign="center" align="center"><img src="images/grad.png" alt="nthu" width="220" height="250"></td>
            </td>
            <td width="85%" style="padding:20px;vertical-align:middle">
                  <a class="tog" href="#triplet">
                    <papertitle>Rotate to Attend: Convolutional Triplet Attention Module</papertitle>
                  </a>
                  <br>
                  <strong>Diganta Misra<sup>*</sup></strong>,
				  <a href="https://scholar.google.com/citations?user=1YmHR6MAAAAJ&hl=en" target="_blank">Trikay Nalamada</a><sup>*</sup>,
				  <a href="https://iyaja.github.io/" target="_blank">Ajay Uppili Arasanipalai</a><sup>*</sup>,
				  <a href="https://andrew-qibin.github.io/homepage/" target="_blank">Qibin Hou</a>
				  <br>
                  <p></p>
                  <em>WACV, 2021</em>
                  <div class="paper" id="wacv2021">
                    <a class="tog" href="https://github.com/landskape-ai/triplet-attention" target="_blank">project</a> /
                    <a href="https://openaccess.thecvf.com/content/WACV2021/papers/Misra_Rotate_to_Attend_Convolutional_Triplet_Attention_Module_WACV_2021_paper.pdf" target="_blank">paper</a> /
					<a href="https://openaccess.thecvf.com/content/WACV2021/supplemental/Misra_Rotate_to_Attend_WACV_2021_supplemental.pdf" target="_blank">supplementary</a> /
					<a href="https://youtu.be/ZW9_2bNF1zo" target="_blank">video</a> /
                      <a class="tog" href="javascript:toggleblock('wacv_abs')">abstract</a> / 
                      <a class="tog" href="javascript:toggleblock('wacv_bib')">bibtex</a>
                      <p align="justify">
                        <i id="wacv_abs">
                            Benefiting from the capability of building interdependencies among channels or spatial locations, attention mechanisms have been extensively studied and broadly
							used in a variety of computer vision tasks recently. In
							this paper, we investigate light-weight but effective attention mechanisms and present <b>triplet attention</b>, a novel
							method for computing attention weights by capturing cross-dimension interaction using a three-branch structure. For
							an input tensor, triplet attention builds inter-dimensional
							dependencies by the rotation operation followed by residual transformations and encodes inter-channel and spatial
							information with negligible computational overhead. Our
							method is simple as well as efficient and can be easily
							plugged into classic backbone networks as an add-on module. We demonstrate the effectiveness of our method on
							various challenging tasks including image classification on
							ImageNet-1k and object detection on MSCOCO and PASCAL VOC datasets. Furthermore, we provide extensive insight into the performance of triplet attention by visually
							inspecting the GradCAM and GradCAM++ results. The
							empirical evaluation of our method supports our intuition
							on the importance of capturing dependencies across dimensions when computing attention weights.
                        </i>
                      </p>
                      <bibtext xml:space="preserve" id="wacv_bib">
						@inproceedings{misra2021rotate, <br />
						title={Rotate to attend: Convolutional triplet attention module}, <br />
						author={Misra, Diganta and Nalamada, Trikay and Arasanipalai, Ajay Uppili and Hou, Qibin}, <br />
						booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision}, <br />
						pages={3139--3148}, <br />
						year={2021} <br />
						}
                            
                        </bibtext>
					<img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/landskapeai/triplet-attention?style=social">
					&emsp;
					<img alt="GitHub forks" src="https://img.shields.io/github/forks/landskapeai/triplet-attention?style=social">
					&emsp;
					<a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=LwiJwNYAAAAJ&citation_for_view=LwiJwNYAAAAJ:UeHWp8X0CEIC" target="_blank"><img src="https://img.shields.io/badge/Citations-181-lightgrey.svg?style=social&logo=Google Scholar"></a>
                    </div>              
                    <script language="JavaScript">hideblock('wacv_bib');hideblock('wacv_abs');</script>
            </td>
			</tr>
			<br>

			<td width="15%" valign="center" align="center"><img src="images/app.drawio.png" alt="nthu" width="400" height="170"></td>
            </td>
            <td width="85%" style="padding:20px;vertical-align:middle">
                  <a class="tog" href="#app2022">
                    <papertitle>APP: Anytime Progressive Pruning &emsp14; <span style="color:red">New!</span></papertitle>
                  </a>
                  <br>
                  <strong>Diganta Misra<sup>*</sup></strong>,
				  <a href="https://bharat-runwal.github.io/" target="_blank">Bharat Runwal<sup>*</sup></a>,
				<a href="https://tianlong-chen.github.io/about/" target="_blank">Tianlong Chen</a>,
				<a href="https://spark.adobe.com/page/CAdrFMJ9QeI2y/" target="_blank">Zhangyang Wang</a>,
				<a href="https://sites.google.com/site/irinarish/" target="_blank">Irina Rish</a>
				  <br>
                  <p></p>
                  <em>DyNN workshop at ICML,2022</em>
				  <br>
				  <em>SNN, 2022</em>
				  <br>
				  <em>CLL workshop at ACML, 2022</em>
				  <br>
				  <em>SlowDNN workshop, 2023</em>
                  <div class="paper" id="app2022">
                    <a class="tog" href="https://github.com/landskape-ai/Progressive-Pruning" target="_blank">project</a> /
                    <a href="https://arxiv.org/abs/2204.01640" target="_blank">paper</a> /
					<a href="https://landskape.ai/publication/app/" target="_blank">webpage</a> /
                      <a class="tog" href="javascript:toggleblock('app_abs')">abstract</a> / 
                      <a class="tog" href="javascript:toggleblock('app_bib')">bibtex</a>
                      <p align="justify">
                        <i id="app_abs">
                            With the latest advances in deep learning, there has been a lot of focus on the online learning paradigm due to its relevance in practical settings. Although many methods have been investigated for optimal learning settings in scenarios where the data stream is continuous over time, sparse networks training in such settings have often been overlooked. In this paper, we explore the problem of training a neural network with a target sparsity in a particular case of online learning: the anytime learning at macroscale paradigm (ALMA). We propose a novel way of progressive pruning, referred to as \textit{Anytime Progressive Pruning} (APP); the proposed approach significantly outperforms the baseline dense and Anytime OSP models across multiple architectures and datasets under short, moderate, and long-sequence training. Our method, for example, shows an improvement in accuracy of $\approx 7\%$ and a reduction in the generalization gap by $\approx 22\%$, while being $\approx 1/3$ rd the size of the dense baseline model in few-shot restricted imagenet training. We further observe interesting nonmonotonic transitions in the generalization gap in the high number of megabatches-based ALMA. The code and experiment dashboards can be accessed at \url{https://github.com/landskape-ai/Progressive-Pruning} and \url{https://wandb.ai/landskape/APP}, respectively.
                        </i>
                      </p>
                      <bibtext xml:space="preserve" id="app_bib">
						@misc{misra2022app,<br />
							title={APP: Anytime Progressive Pruning},<br />
							author={Diganta Misra and Bharat Runwal and Tianlong Chen and Zhangyang Wang and Irina Rish},<br />
							year={2022},<br />
							eprint={2204.01640},<br />
							archivePrefix={arXiv},<br />
							primaryClass={cs.LG}<br />
						}
                            
                        </bibtext>
					
					<a href="https://bluejeans.com/playback/s/jFFH3X6y1UxAuLKW5kau2WRQHbMojKfpaySI6xGRQ56lqpaEmbdKsC9wBwWTJUag" target="_blank">NSL presentation</a> /
					<a href="https://youtu.be/GbHpaqwkgxE?t=924" target="_blank">MLC Research Jam #8</a> /
					<br>
					<a href="https://youtu.be/tXWcrwOwhFE?t=573" target="_blank">MLC Research Jam #9</a> /
					<a href="https://youtu.be/EZS2PzfyfXY" target="_blank">Continual AI Seminar</a>
					<br>
					<br>
					<img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/landskape-ai/Progressive-Pruning?style=social">
					&emsp;
					<img alt="GitHub Repo forks" src="https://img.shields.io/github/forks/landskape-ai/Progressive-Pruning?style=social">
					&emsp;
					<a href="https://wandb.ai/landskape/APP" target="_blank"><img src="https://img.shields.io/badge/Dashboard-WandB?style=flat&color=black&logo=data:image/png;base64,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"></a>
                    </div>              
                    <script language="JavaScript">hideblock('app_bib');hideblock('app_abs');</script>
            </td>
			</tr>

			<td width="15%" valign="center" align="center"><img src="images/scole.drawio.png" alt="nthu" width="350" height="80"></td>
            </td>
            <td width="85%" style="padding:20px;vertical-align:middle">
                  <a class="tog" href="#scole2022">
                    <papertitle>Scaling the Number of Tasks in Continual Learning&emsp14; <span style="color:red">New!</span></papertitle>
                  </a>
                  <br>
				  <a href="https://tlesort.github.io/" target="_blank">Timoth√©e Lesort</a>,
				  <a href="https://scholar.google.de/citations?user=mqLVUGgAAAAJ&hl=de" target="_blank">Oleksiy Ostapenko</a>,
				  <strong>Diganta Misra</strong>,
				  <a href="https://mila.quebec/personne/32484/" target="_blank">Md Rifat Arefin</a>,
				  <a href="https://scholar.google.com/citations?user=IwBx73wAAAAJ&hl=fr" target="_blank">Pau Rodriguez</a>,
				  <a href="http://www.cs.toronto.edu/~lcharlin/" target="_blank">Laurent Charlin</a>,
				  <a href="https://sites.google.com/site/irinarish/" target="_blank">Irina Rish</a>
				  <br>
                  <p></p>
                  <em>CoLLAs workshop, 2022</em>
                  <div class="paper" id="scole2022">
					  <a href="https://arxiv.org/abs/2207.04543" target="_blank">paper</a> /
                      <a class="tog" href="javascript:toggleblock('scole_abs')">abstract</a> /
					  <a class="tog" href="javascript:toggleblock('scole_bib')">bibtex</a>
                      <p align="justify">
                        <i id="scole_abs">
                            Standard gradient descent algorithms applied to sequences of tasks are known to produce catastrophic forgetting in deep neural networks. When trained on a new task in a sequence, the model updates its parameters on the current task, forgetting past knowledge. This article explores scenarios where we scale the number of tasks in a finite environment. Those scenarios are composed of a long sequence of tasks with reoccurring data. We show that in such setting, stochastic gradient descent can learn, progress, and converge to a solution that according to existing literature needs a continual learning algorithm. In other words, we show that the model performs knowledge retention and accumulation without specific memorization mechanisms. We propose a new experimentation framework, SCoLe (Scaling Continual Learning), to study the knowledge retention and accumulation of algorithms in potentially infinite sequences of tasks. To explore this setting, we performed a large number of experiments on sequences of 1,000 tasks to better understand this new family of settings. We also propose a slight modifications to the vanilla stochastic gradient descent to facilitate continual learning in this setting. The SCoLe framework represents a good simulation of practical training environments with reoccurring situations and allows the study of convergence behavior in long sequences. Our experiments show that previous results on short scenarios cannot always be extrapolated to longer scenarios.
                        </i>
                      </p>

					  <bibtext xml:space="preserve" id="scole_bib">
						@article{lesort2022scaling,<br />
							title   = {Scaling the Number of Tasks in Continual Learning},<br />
							author  = {Timoth√©e Lesort and Oleksiy Ostapenko and Diganta Misra and Md Rifat Arefin and Pau Rodr√≠guez and Laurent Charlin and Irina Rish},<br />
							year    = {2022},<br />
							journal = {arXiv preprint arXiv: Arxiv-2207.04543}<br />
						}
                        </bibtext>

					<br>
                    </div>              
                    <script language="JavaScript">hideblock('scole_abs');;hideblock('scole_bib');</script>
            </td>
			</tr>


			<td width="15%" valign="center" align="center"><img src="https://media.githubusercontent.com/media/google/BIG-bench/main/bigbench/benchmark_tasks/tense/results/plot__tense__aggregate__exact_str_match.png" alt="nthu" width="230" height="210"></td>
            </td>
            <td width="85%" style="padding:20px;vertical-align:middle">
                  <a class="tog" href="#big2022">
                    <papertitle>Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models&emsp14; <span style="color:red">New!</span></papertitle>
                  </a>
                  <br>
                  <strong>Diganta Misra</strong>,
				  <a href="https://in.linkedin.com/in/mukundvarmat" target="_blank">Mukund Varma T.</a>,
				  Multiple authors
				  <br>
                  <p></p>
                  <em>Preprint, 2022</em>
                  <div class="paper" id="big2022">
                    <a class="tog" href="https://github.com/google/BIG-bench" target="_blank">project</a> /
                    <a href="https://arxiv.org/abs/2206.04615" target="_blank">paper</a> /
                      <a class="tog" href="javascript:toggleblock('big_abs')">abstract</a> / 
                      <a class="tog" href="javascript:toggleblock('big_bib')">bibtex</a>
                      <p align="justify">
                        <i id="big_abs">
                            Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 442 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI's GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide a strong baseline. Findings include: model performance and calibration both improve with scale, but are poor in absolute terms (and when compared with rater performance); performance is remarkably similar across model classes, though with benefits from sparsity; tasks that improve gradually and predictably commonly involve a large knowledge or memorization component, whereas tasks that exhibit "breakthrough" behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting.
                        </i>
                      </p>
                      <bibtext xml:space="preserve" id="big_bib">
						@article{srivastava2022beyond,<br />
							title   = {Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models},<br />
							author  = {Aarohi Srivastava and Abhinav Rastogi and Abhishek Rao and Abu Awal Md Shoeb and Abubakar Abid and Adam Fisch and Adam R. Brown and Adam Santoro and Aditya Gupta and Adri√† Garriga-Alonso and Agnieszka Kluska and Aitor Lewkowycz and Akshat Agarwal and Alethea Power and Alex Ray and Alex Warstadt and Alexander W. Kocurek and Ali Safaya and Ali Tazarv and Alice Xiang and Alicia Parrish and Allen Nie and Aman Hussain and Amanda Askell and Amanda Dsouza and Ameet Rahane and Anantharaman S. Iyer and Anders Andreassen and Andrea Santilli and Andreas Stuhlm√ºller and Andrew Dai and Andrew La and Andrew Lampinen and Andy Zou and Angela Jiang and Angelica Chen and Anh Vuong and Animesh Gupta and Anna Gottardi and Antonio Norelli and Anu Venkatesh and Arash Gholamidavoodi and Arfa Tabassum and Arul Menezes and Arun Kirubarajan and Asher Mullokandov and Ashish Sabharwal and Austin Herrick and Avia Efrat and Aykut Erdem and Ayla Karaka≈ü and B. Ryan Roberts and Bao Sheng Loe and Barret Zoph and Bart≈Çomiej Bojanowski and Batuhan √ñzyurt and Behnam Hedayatnia and Behnam Neyshabur and Benjamin Inden and Benno Stein and Berk Ekmekci and Bill Yuchen Lin and Blake Howald and Cameron Diao and Cameron Dour and Catherine Stinson and Cedrick Argueta and C√©sar Ferri Ram√≠rez and Chandan Singh and Charles Rathkopf and Chenlin Meng and Chitta Baral and Chiyu Wu and Chris Callison-Burch and Chris Waites and Christian Voigt and Christopher D. Manning and Christopher Potts and Cindy Ramirez and Clara E. Rivera and Clemencia Siro and Colin Raffel and Courtney Ashcraft and Cristina Garbacea and Damien Sileo and Dan Garrette and Dan Hendrycks and Dan Kilman and Dan Roth and Daniel Freeman and Daniel Khashabi and Daniel Levy and Daniel Mosegu√≠ Gonz√°lez and Danny Hernandez and Danqi Chen and Daphne Ippolito and Dar Gilboa and David Dohan and David Drakard and David Jurgens and Debajyoti Datta and Deep Ganguli and Denis Emelin and Denis Kleyko and Deniz Yuret and Derek Chen and Derek Tam and Dieuwke Hupkes and Diganta Misra and Dilyar Buzan and Dimitri Coelho Mollo and Diyi Yang and Dong-Ho Lee and Ekaterina Shutova and Ekin Dogus Cubuk and Elad Segal and Eleanor Hagerman and Elizabeth Barnes and Elizabeth Donoway and Ellie Pavlick and Emanuele Rodola and Emma Lam and Eric Chu and Eric Tang and Erkut Erdem and Ernie Chang and Ethan A. Chi and Ethan Dyer and Ethan Jerzak and Ethan Kim and Eunice Engefu Manyasi and Evgenii Zheltonozhskii and Fanyue Xia and Fatemeh Siar and Fernando Mart√≠nez-Plumed and Francesca Happ√© and Francois Chollet and Frieda Rong and Gaurav Mishra and Genta Indra Winata and Gerard de Melo and Germ√°n Kruszewski and Giambattista Parascandolo and Giorgio Mariani and Gloria Wang and Gonzalo Jaimovitch-L√≥pez and Gregor Betz and Guy Gur-Ari and Hana Galijasevic and Hannah Kim and Hannah Rashkin and Hannaneh Hajishirzi and Harsh Mehta and Hayden Bogar and Henry Shevlin and Hinrich Sch√ºtze and Hiromu Yakura and Hongming Zhang and Hugh Mee Wong and Ian Ng and Isaac Noble and Jaap Jumelet and Jack Geissinger and Jackson Kernion and Jacob Hilton and Jaehoon Lee and Jaime Fern√°ndez Fisac and James B. Simon and James Koppel and James Zheng and James Zou and Jan Koco≈Ñ and Jana Thompson and Jared Kaplan and Jarema Radom and Jascha Sohl-Dickstein and Jason Phang and Jason Wei and Jason Yosinski and Jekaterina Novikova and Jelle Bosscher and Jennifer Marsh and Jeremy Kim and Jeroen Taal and Jesse Engel and Jesujoba Alabi and Jiacheng Xu and Jiaming Song and Jillian Tang and Joan Waweru and John Burden and John Miller and John U. Balis and Jonathan Berant and J√∂rg Frohberg and Jos Rozen and Jose Hernandez-Orallo and Joseph Boudeman and Joseph Jones and Joshua B. Tenenbaum and Joshua S. Rule and Joyce Chua and Kamil Kanclerz and Karen Livescu and Karl Krauth and Karthik Gopalakrishnan and Katerina Ignatyeva and Katja Markert and Kaustubh D. Dhole and Kevin Gimpel and Kevin Omondi and Kory Mathewson and Kristen Chiafullo and Ksenia Shkaruta and Kumar Shridhar and Kyle McDonell and Kyle Richardson and Laria Reynolds and Leo Gao and Li Zhang and Liam Dugan and Lianhui Qin and Lidia Contreras-Ochando and Louis-Philippe Morency and Luca Moschella and Lucas Lam and Lucy Noble and Ludwig Schmidt and Luheng He and Luis Oliveros Col√≥n and Luke Metz and L√ºtfi Kerem ≈ûenel and Maarten Bosma and Maarten Sap and Maartje ter Hoeve and Madotto Andrea and Maheen Farooqi and Manaal Faruqui and Mantas Mazeika and Marco Baturan and Marco Marelli and Marco Maru and Maria Jose Ram√≠rez Quintana and Marie Tolkiehn and Mario Giulianelli and Martha Lewis and Martin Potthast and Matthew L. Leavitt and Matthias Hagen and M√°ty√°s Schubert and Medina Orduna Baitemirova and Melody Arnaud and Melvin McElrath and Michael A. Yee and Michael Cohen and Michael Gu and Michael Ivanitskiy and Michael Starritt and Michael Strube and Micha≈Ç Swƒôdrowski and Michele Bevilacqua and Michihiro Yasunaga and Mihir Kale and Mike Cain and Mimee Xu and Mirac Suzgun and Mo Tiwari and Mohit Bansal and Moin Aminnaseri and Mor Geva and Mozhdeh Gheini and Mukund Varma T and Nanyun Peng and Nathan Chi and Nayeon Lee and Neta Gur-Ari Krakover and Nicholas Cameron and Nicholas Roberts and Nick Doiron and Nikita Nangia and Niklas Deckers and Niklas Muennighoff and Nitish Shirish Keskar and Niveditha S. Iyer and Noah Constant and Noah Fiedel and Nuan Wen and Oliver Zhang and Omar Agha and Omar Elbaghdadi and Omer Levy and Owain Evans and Pablo Antonio Moreno Casares and Parth Doshi and Pascale Fung and Paul Pu Liang and Paul Vicol and Pegah Alipoormolabashi and Peiyuan Liao and Percy Liang and Peter Chang and Peter Eckersley and Phu Mon Htut and Pinyu Hwang and Piotr Mi≈Çkowski and Piyush Patil and Pouya Pezeshkpour and Priti Oli and Qiaozhu Mei and Qing Lyu and Qinlang Chen and Rabin Banjade and Rachel Etta Rudolph and Raefer Gabriel and Rahel Habacker and Ram√≥n Risco Delgado and Rapha√´l Milli√®re and Rhythm Garg and Richard Barnes and Rif A. Saurous and Riku Arakawa and Robbe Raymaekers and Robert Frank and Rohan Sikand and Roman Novak and Roman Sitelew and Ronan LeBras and Rosanne Liu and Rowan Jacobs and Rui Zhang and Ruslan Salakhutdinov and Ryan Chi and Ryan Lee and Ryan Stovall and Ryan Teehan and Rylan Yang and Sahib Singh and Saif M. Mohammad and Sajant Anand and Sam Dillavou and Sam Shleifer and Sam Wiseman and Samuel Gruetter and Samuel R. Bowman and Samuel S. Schoenholz and Sanghyun Han and Sanjeev Kwatra and Sarah A. Rous and Sarik Ghazarian and Sayan Ghosh and Sean Casey and Sebastian Bischoff and Sebastian Gehrmann and Sebastian Schuster and Sepideh Sadeghi and Shadi Hamdan and Sharon Zhou and Shashank Srivastava and Sherry Shi and Shikhar Singh and Shima Asaadi and Shixiang Shane Gu and Shubh Pachchigar and Shubham Toshniwal and Shyam Upadhyay and Shyamolima and Debnath and Siamak Shakeri and Simon Thormeyer and Simone Melzi and Siva Reddy and Sneha Priscilla Makini and Soo-Hwan Lee and Spencer Torene and Sriharsha Hatwar and Stanislas Dehaene and Stefan Divic and Stefano Ermon and Stella Biderman and Stephanie Lin and Stephen Prasad and Steven T. Piantadosi and Stuart M. Shieber and Summer Misherghi and Svetlana Kiritchenko and Swaroop Mishra and Tal Linzen and Tal Schuster and Tao Li and Tao Yu and Tariq Ali and Tatsu Hashimoto and Te-Lin Wu and Th√©o Desbordes and Theodore Rothschild and Thomas Phan and Tianle Wang and Tiberius Nkinyili and Timo Schick and Timofei Kornev and Timothy Telleen-Lawton and Titus Tunduny and Tobias Gerstenberg and Trenton Chang and Trishala Neeraj and Tushar Khot and Tyler Shultz and Uri Shaham and Vedant Misra and Vera Demberg and Victoria Nyamai and Vikas Raunak and Vinay Ramasesh and Vinay Uday Prabhu and Vishakh Padmakumar and Vivek Srikumar and William Fedus and William Saunders and William Zhang and Wout Vossen and Xiang Ren and Xiaoyu Tong and Xinyi Wu and Xudong Shen and Yadollah Yaghoobzadeh and Yair Lakretz and Yangqiu Song and Yasaman Bahri and Yejin Choi and Yichi Yang and Yiding Hao and Yifu Chen and Yonatan Belinkov and Yu Hou and Yufang Hou and Yuntao Bai and Zachary Seid and Zhao Xinran and Zhuoye Zhao and Zijian Wang and Zijie J. Wang and Zirui Wang and Ziyi Wu},<br />
							year    = {2022},<br />
							journal = {arXiv preprint arXiv: Arxiv-2206.04615}<br />
						  }
                            
                        </bibtext>


					<a href="https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/tense" target="_blank">Tense task</a>
					<br>
					<br>
					<img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/google/BIG-bench?style=social">
					&emsp;
					<img alt="GitHub forks" src="https://img.shields.io/github/forks/google/BIG-bench?style=social">
                    &emsp;
					<a href="https://scholar.google.com/citations?view_op=view_citation&hl=en&user=LwiJwNYAAAAJ&citation_for_view=LwiJwNYAAAAJ:aqlVkmm33-oC" target="_blank"><img src="https://img.shields.io/badge/Citations-49-lightgrey.svg?style=social&logo=Google Scholar"></a>
                    </div>             
                    <script language="JavaScript">hideblock('big_bib');hideblock('big_abs');</script>
            </td>
			</tr>


			<td width="15%" valign="center" align="center"><img src="images/ga.jpg" alt="nthu" width="330" height="150"></td>
            </td>
            <td width="85%" style="padding:20px;vertical-align:middle">
                  <a class="tog" href="#ga">
                    <papertitle>Genetic Algorithm Optimized Inkjet Printed Electromagnetic Absorber on Paper Substrate</papertitle>
                  </a>
                  <br>
                  <strong>Diganta Misra</strong>,
				  Rahul Pelluri,
				  <a href="https://scholar.google.com/citations?user=iK_wjZsAAAAJ&hl=en" target="_blank">Vijay Kumar Verma</a>,
				  <a href="https://scholar.google.com/citations?user=iGAvOmEAAAAJ&hl=en" target="_blank">Bhargav Appasani</a>,
				  Nisha Gupta
				  <br>
                  <p></p>
                  <em>IEEE AESPC, 2018</em>
                  <div class="paper" id="ga2018">
                    <a href="data/misra2018.pdf" target="_blank">paper</a> /
                      <a class="tog" href="javascript:toggleblock('ga_abs')">abstract</a> / 
                      <a class="tog" href="javascript:toggleblock('ga_bib')">bibtex</a>
                      <p align="justify">
                        <i id="ga_abs">
                            Printable electronics based electromagnetic absorbers are receiving increasing attention of the electromagnetic community because of their unprecedented advantages. This paper presents the design of printable electromagnetic absorbers for the X band. The design of the absorber is optimized using the Genetic Algorithm (GA) to enhance the absorptivity and the absorption bandwidth. The design involves the placement of several square-shaped conductive ink at optimal locations on the paper substrate such that desired absorption characteristics are obtained. Simulations are carried out using the HFSS simulation software. The optimized structure offers an absorptivity of more than 90% in the X band thereby proving to be a viable solution for stealth applications.
                        </i>
                      </p>
                      <bibtext xml:space="preserve" id="ga_bib">
						@inproceedings{misra2018genetic, <br />
						title={Genetic Algorithm Optimized Inkjet Printed Electromagnetic Absorber on Paper Substrate}, <br />
						author={Misra, Diganta and Pelluri, Rahul and Verma, Vijay Kumar and Appasani, Bhargav and Gupta, Nisha}, <br />
						booktitle={2018 International Conference on Applied Electromagnetics, Signal Processing and Communication (AESPC)}, <br />
						volume={1}, <br />
						pages={1--3}, <br />
						year={2018}, <br />
						organization={IEEE} <br />
						}
                            
                        </bibtext>
						<a href="https://scholar.google.com/scholar?oi=bibs&hl=en&cites=1834772988619324668" target="_blank"><img src="https://img.shields.io/badge/Citations-1-lightgrey.svg?style=social&logo=Google Scholar"></a>
					</div>              
                    <script language="JavaScript">hideblock('ga_bib');hideblock('ga_abs');</script>
            </td>
		</tr>

		<td width="15%" valign="center" align="center"><img src="images/cc.png" alt="nthu" width="330" height="100"></td>
            </td>
            <td width="85%" style="padding:20px;vertical-align:middle">
                  <a class="tog" href="#cc">
                    <papertitle>Convoluted Cosmos: Classifying Galaxy Images Using Deep Learning</papertitle>
                  </a>
                  <br>
                  <strong>Diganta Misra</strong>,
				  <a href="https://scholar.google.co.in/citations?user=166bepcAAAAJ&hl=en" target="_blank">Sachi Nandan Mohanty</a>,
				  <a href="https://scholar.google.co.uk/citations?user=gt9wdUwAAAAJ&hl=en&oi=ao" target="_blank">Mohit Agarwal</a>,
				  <a href="https://scholar.google.co.uk/citations?user=ZYXq9xoAAAAJ&hl=en&oi=ao" target="_blank">Suneet K Gupta</a>
				  <br>
                  <p></p>
                  <em>Springer ICDMAI, 2019 (Proceedings of the AISC)</em>
                  <div class="paper" id="ga2018">
                    <a href="https://link.springer.com/chapter/10.1007/978-981-32-9949-8_40#chapter-info" target="_blank">paper</a> /
                      <a class="tog" href="javascript:toggleblock('cc_abs')">abstract</a> / 
                      <a class="tog" href="javascript:toggleblock('cc_bib')">bibtex</a>
                      <p align="justify">
                        <i id="cc_abs">
                            In this paper, a deep learning-based approach has been developed to classify the images of galaxies into three major categories, namely, elliptical, spiral, and irregular. The classifier successfully classified the images with an accuracy of 97.3958%, which outperformed conventional classifiers like Support Vector Machine and Naive Bayes. The convolutional neural network architecture involves one input convolution layer having 16 filters, followed by 4 hidden layers, 1 penultimate dense layer, and an output Softmax layer. The model was trained on 4614 images for 200 epochs using NVIDIA-DGX-1 Tesla-V100 Supercomputer machine and was subsequently tested on new images to evaluate its robustness and accuracy.
                        </i>
                      </p>
                      <bibtext xml:space="preserve" id="cc_bib">
						@incollection{misra2020convoluted,<br />
							title={Convoluted cosmos: classifying galaxy images using deep learning},<br />
							author={Misra, Diganta and Mohanty, Sachi Nandan and Agarwal, Mohit and Gupta, Suneet K},<br />
							booktitle={Data Management, Analytics and Innovation},<br />
							pages={569--579},<br />
							year={2020},<br />
							publisher={Springer}<br />
						  }

					</bibtext>
					<a href="https://scholar.google.com/scholar?oi=bibs&hl=en&cites=15150003569569138477" target="_blank"><img src="https://img.shields.io/badge/Citations-10-lightgrey.svg?style=social&logo=Google Scholar"></a>
				</div>              
				<script language="JavaScript">hideblock('cc_bib');hideblock('cc_abs');</script>
		</td>
	</tr>
          
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Research under progress/ under review</heading>
          </td>
        </tr>
      </tbody></table>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

			<!-- <td width="25%" valign="center" align="center"><iframe src="https://giphy.com/embed/wHYLDtqUn6Aqq2jVmx" width="160" height="160" frameBorder="0" class="giphy-embed" allowFullScreen></iframe></td>
              <td style="padding:20px;width:75%;vertical-align:middle">
                <a class="tog" id="vit">
                  <papertitle>Active channel selection in Vision Transformers</papertitle>
                </a>
                <br>
                <strong>Diganta Misra</strong>,
				<a href="https://bharat-runwal.github.io/" target="_blank">Bharat Runwal</a>
                <br>
                <p></p>
              </td>
            </tr> -->

			<td width="25%" valign="center" align="center"><iframe src="https://giphy.com/embed/U9HWpUQ17HYZFpqlff" width="160" height="160" frameBorder="0" class="giphy-embed" allowFullScreen></iframe></td>
			<td style="padding:20px;width:75%;vertical-align:middle">
				<a class="tog" id="spade">
				<papertitle>SPADE</papertitle>
				</a>
				<br>
				<strong>Diganta Misra</strong>,
				<a href="https://bharat-runwal.github.io/" target="_blank">Bharat Runwal (Landskape AI)</a>,
				<a href="https://gkdz.org/" target="_blank">Gintare Karolina Dziugaite (Google Brain)</a>
				<br>
				<p></p>
			</td>
			</tr>

			<!-- <td width="25%" valign="center" align="center"><iframe src="https://giphy.com/embed/U9HWpUQ17HYZFpqlff" width="160" height="160" frameBorder="0" class="giphy-embed" allowFullScreen></iframe></td>
			<td style="padding:20px;width:75%;vertical-align:middle">
				<a class="tog" id="shapely">
				<papertitle>Shapely Tickets</papertitle>
				</a>
				<br>
				<strong>Diganta Misra</strong>,
				<a href="https://bharat-runwal.github.io/" target="_blank">Bharat Runwal</a>,
				<a href="http://nsaphra.github.io/" target="_blank">Naomi Saphra</a>,
				<a href="https://bellecarrell.github.io/" target="_blank">Annabelle Carrell</a>
				<br>
				<p></p>
			</td>
			</tr> -->

			<!-- <td width="25%" valign="center" align="center"><iframe src="https://giphy.com/embed/1AdyMQSFudW0rksSci" width="160" height="160" frameBorder="0" class="giphy-embed" allowFullScreen></iframe></td>
			<td style="padding:20px;width:75%;vertical-align:middle">
				<a class="tog" id="APP">
				<papertitle>Robust Generalisation(task variance)>Robust Generalisation(config variance)</papertitle>
				</a>
				<br>
				<strong>Diganta Misra</strong>,
				<a href="https://bharat-runwal.github.io/" target="_blank">Bharat Runwal</a>,
				<a href="https://bknyaz.github.io/" target="_blank">Boris Knyazev</a>,
				<a href="https://sites.google.com/view/marwaelhalabi/home" target="_blank">Marwa El Halabi</a>,
				<a href="https://www.cyanogenoid.com/" target="_blank">Yan Zhang</a>
				<br>
				<p></p>
			</td>
			</tr> -->

			<td width="25%" valign="center" align="center"><iframe src="https://giphy.com/embed/l3q2ypdW3QmdYqGLC" width="160" height="160" frameBorder="0" class="giphy-embed" allowFullScreen></iframe></td>
			<td style="padding:20px;width:75%;vertical-align:middle">
				<a class="tog" id="ghn">
				<papertitle>Understanding effective robustness of Visual Language Models</papertitle>
				</a>
				<br>
				<strong>Diganta Misra</strong>,
				<a href="https://cseweb.ucsd.edu/~yaq007/" target="_blank">Yao Qin (Google Research, UCSB)</a>
				<br>
				<p></p>
			</td>
			</tr>

			<!-- <td width="25%" valign="center" align="center"><iframe src="https://giphy.com/embed/hVZgxJJHNUwUYTb7p9" width="160" height="160" frameBorder="0" class="giphy-embed" allowFullScreen></iframe></td>
			<td style="padding:20px;width:75%;vertical-align:middle">
				<a class="tog" id="influence">
				<papertitle>Influence != Hardness</papertitle>
				</a>
				<br>
				<strong>Diganta Misra</strong>,
				<a href="https://bharat-runwal.github.io/" target="_blank">Bharat Runwal</a>
				<br>
				<p></p>
			</td>
			</tr> -->

			<!-- <td width="25%" valign="center" align="center"><iframe src="https://giphy.com/embed/FOdT4x8rHYAqQ" width="160" height="160" frameBorder="0" class="giphy-embed" allowFullScreen></iframe></td>
			<td style="padding:20px;width:75%;vertical-align:middle">
				<a class="tog" id="psnr">
				<papertitle>Multi-device denoising using implicit distilled exits</papertitle>
				</a>
				<br>
				<a href="https://www.cse.iitb.ac.in/~richeek/" target="_blank">Richeek Das</a>,
				<a href="https://bharat-runwal.github.io/" target="_blank">Bharat Runwal</a>,
				<strong>Diganta Misra</strong>
				<br>
				<p></p>
			</td>
			</tr> -->

			<!-- <td width="25%" valign="center" align="center"><iframe src="https://giphy.com/embed/5kFbMBOEdWjg1nItoG" width="160" height="160" frameBorder="0" class="giphy-embed" allowFullScreen></iframe></td>
			<td style="padding:20px;width:75%;vertical-align:middle">
				<a class="tog" id="control">
				<papertitle>Exit strategy for controlled pruning</papertitle>
				</a>
				<br>
				<strong>Diganta Misra</strong>,
				<a href="https://bharat-runwal.github.io/" target="_blank">Bharat Runwal</a>
				<br>
				<p></p>
			</td>
			</tr> -->

			<!-- <td width="25%" valign="center" align="center"><iframe src="https://giphy.com/embed/xUOrwmkGgkqEIE3840" width="160" height="160" frameBorder="0" class="giphy-embed" allowFullScreen></iframe></td>
			<td style="padding:20px;width:75%;vertical-align:middle">
				<a class="tog" id="active">
				<papertitle>Active Anytime Learning at Macroscale</papertitle>
				</a>
				<br>
				<strong>Diganta Misra</strong>,
				<a href="https://bharat-runwal.github.io/" target="_blank">Bharat Runwal</a>,
				<a href="https://tlesort.github.io/" target="_blank">Timoth√©e Lesort</a>,
				<a href="https://www.cs.mcgill.ca/~lpagec/" target="_blank">Lucas Caccia</a>
				<br>
				<p></p>
			</td>
			</tr> -->


			<td width="25%" valign="center" align="center"><iframe src="https://giphy.com/embed/9DaWX2otgnhyHQoY8V" width="160" height="160" frameBorder="0" class="giphy-embed" allowFullScreen></iframe></td>
			<td style="padding:20px;width:75%;vertical-align:middle">
				<a class="tog" id="pred">
				<papertitle>Minimal exit depth as a proxy for estimating miscalibration of neural networks.</papertitle>
				</a>
				<br>
				<a href="https://bharat-runwal.github.io/" target="_blank">Bharat Runwal (Landskape AI)</a>,
				<strong>Diganta Misra</strong>
				<br>
				<p></p>
			</td>
			</tr>

			<!-- <td width="25%" valign="center" align="center"><iframe src="https://giphy.com/embed/LoyGk1omC4Vto1btFo" width="160" height="160" frameBorder="0" class="giphy-embed" allowFullScreen></iframe></td>
			<td style="padding:20px;width:75%;vertical-align:middle">
				<a class="tog" id="lc">
				<papertitle>Learning Curves for Continual Learning: Trade-off in generalization and forgetting in replay methods</papertitle>
				</a>
				<br>
				<strong>Diganta Misra</strong>,
				<a href="https://scholar.google.de/citations?user=mqLVUGgAAAAJ&hl=de" target="_blank">Oleksiy Ostapenko</a>,
				<a href="https://tlesort.github.io/" target="_blank">Timoth√©e Lesort</a>,
				<a href="https://sites.google.com/site/irinarish/" target="_blank">Irina Rish</a>
				<br>
				<p></p>
			</td>
			</tr> -->

			<!-- <td width="25%" valign="center" align="center"><iframe src="https://giphy.com/embed/aXR1Rw85WmcLF3fkkc" width="160" height="160" frameBorder="0" class="giphy-embed" allowFullScreen></iframe></td>
			<td style="padding:20px;width:75%;vertical-align:middle">
				<a class="tog" id="hyper">
				<papertitle>HyperActivation</papertitle>
				</a>
				<br>
				<a href="https://avivsham.github.io/" target="_blank">Aviv Shamsian</a>,
				<strong>Diganta Misra</strong>,
				<a href="https://bharat-runwal.github.io/" target="_blank">Bharat Runwal</a>
				<br>
				<p></p>
			</td>
			</tr> -->

			<td width="25%" valign="center" align="center"><iframe src="https://giphy.com/embed/ZECNnLnzl72ILOTDhA" width="160" height="160" frameBorder="0" class="giphy-embed" allowFullScreen></iframe></td>
			<td style="padding:20px;width:75%;vertical-align:middle">
				<a class="tog" id="reprogram">
				<papertitle>Continual Reprogramming: Forgetting exempt continual learning with a dictionary of reprogrammers</papertitle>
				</a>
				<br>
				<a href="https://sites.google.com/site/pinyuchenpage/" target="_blank">Pin Yu Chen (IBM)</a>,
				<strong>Diganta Misra</strong>,
				<a href="https://bharat-runwal.github.io/" target="_blank">Bharat Runwal (Landskape AI)</a>,
				<a href="https://sites.google.com/site/irinarish/" target="_blank">Irina Rish (Mila, CERC)</a>
				<br>
				<p></p>
			</td>
			</tr>

			<!-- <td width="25%" valign="center" align="center"><iframe src="https://giphy.com/embed/3o85xHdSmKVjPt4T2o" width="160" height="160" frameBorder="0" class="giphy-embed" allowFullScreen></iframe></td>
			<td style="padding:20px;width:75%;vertical-align:middle">
				<a class="tog" id="APP_2">
				<papertitle>Progressive Pruning and Growing in sequential learning</papertitle>
				</a>
				<br>
				<a href="https://bharat-runwal.github.io/" target="_blank">Bharat Runwal</a>,
				<a href="https://www.cse.iitb.ac.in/~richeek/" target="_blank">Richeek Das</a>,
				<strong>Diganta Misra</strong>
				<br>
				<p></p>
			</td>
			</tr> -->

			<!-- <td width="25%" valign="center" align="center"><iframe src="https://giphy.com/embed/2skyvHAmS7evdz6OH3" width="160" height="160" frameBorder="0" class="giphy-embed" allowFullScreen></iframe></td>
			</td>
			<td style="padding:20px;width:75%;vertical-align:middle">
				<a class="tog" id="correction">
				<papertitle>Progressive weak subnet correction</papertitle>
				</a>
				<br>
				<strong>Diganta Misra</strong>,
				<a href="https://bharat-runwal.github.io/" target="_blank">Bharat Runwal</a>
				<br>
				<p></p>
			</td>
			</tr> -->

			<!-- <td width="25%" valign="center" align="center"><iframe src="https://giphy.com/embed/9xv5O2TXlc30JGhgDl" width="160" height="160" frameBorder="0" class="giphy-embed" allowFullScreen></iframe></td>
			</td>
			<td style="padding:20px;width:75%;vertical-align:middle">
				<a class="tog" id="rmt">
				<papertitle>HT-RMT insights of sparse models</papertitle>
				</a>
				<br>
				<strong>Diganta Misra</strong>
				<br>
				<p></p>
			</td>
			</tr> -->

			<!-- <td width="25%" valign="center" align="center"><iframe src="https://giphy.com/embed/xl2GGfiBBN0EE" width="160" height="160" frameBorder="0" class="giphy-embed" allowFullScreen></iframe></td>
			</td>
			<td style="padding:20px;width:75%;vertical-align:middle">
				<a class="tog" id="clac">
				<papertitle>Curriculum Learning Arithmetic Compositionally</papertitle>
				</a>
				<br>
				<a href="https://mirandrom.github.io/" target="_blank">Andrei Mircea</a>,
				<strong>Diganta Misra</strong>
				<br>
				<p></p>
			</td>
			</tr> -->

			<!-- <td width="25%" valign="center" align="center"><iframe src="https://giphy.com/embed/V0VvUaWozIDTb7fQMz" width="160" height="160" frameBorder="0" class="giphy-embed" allowFullScreen></iframe></td>
			</td>
			<td style="padding:20px;width:75%;vertical-align:middle">
				<a class="tog" id="watermark">
				<papertitle>Watermark degradation in continual learning frameworks</papertitle>
				</a>
				<br>
				<strong>Diganta Misra</strong>,
				<a href="https://bharat-runwal.github.io/" target="_blank">Bharat Runwal</a>
				<br>
				<p></p>
			</td>
			</tr> -->

			<!-- <td width="25%" valign="center" align="center"><iframe src="https://giphy.com/embed/XU567zvlXcuvXGj47m" width="160" height="160" frameBorder="0" class="giphy-embed" allowFullScreen></iframe></td>
			<td style="padding:20px;width:75%;vertical-align:middle">
				<a class="tog" id="flip">
				<papertitle>Cross modal reprogramming for time series forecasting</papertitle></a>
				</a>
				<br>
				<strong>Diganta Misra</strong>
				<br>
				<p></p>
			</td>
			</tr> -->

			<td width="25%" valign="center" align="center"><iframe src="https://giphy.com/embed/RgbzdO1IN7nB9Grb4G" width="160" height="160" frameBorder="0" class="giphy-embed" allowFullScreen></iframe></td>
			<td style="padding:20px;width:75%;vertical-align:middle">
				<a class="tog" id="critical_exit">
				<papertitle>VQA Reprogramming</papertitle></a>
				</a>
				<br>
				<a href="Jay Gala" target="_blank">Jay Gala (AI4Bharat @ IIT Madras)</a>,
				Victor Zhu (UCSD),
				<strong>Diganta Misra</strong>
				<br>
				<p></p>
			</td>
			</tr>

        </tbody></table>
        
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>Open Source Frameworks & Projects</heading> &emsp; <!-- Place this tag where you want the button to render. -->
			<a class="github-button" href="https://github.com/sponsors/digantamisra98" data-color-scheme="no-preference: light; light: light; dark: light;" data-icon="octicon-heart" data-size="large" aria-label="Sponsor @digantamisra98 on GitHub">Sponsor</a>
          </td>
        </tr>
      </tbody></table>
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


        <td width="25%" valign="center" align="center"><img src="images/avalanche.png" alt="nthu" width="180" height="160"></td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a class="tog" href="https://github.com/ContinualAI/avalanche" target="_blank">
              <papertitle>Avalanche: an End-to-End Library for Continual Learning</papertitle>
            </a>
            <br>
            Dec'20 - Present
            <br>
            <br>
            I am an active lead maintainer of the Reproducible Continual Learning framework by Avalanche and
			also actively work on the evaluation framework of Avalanche mainly in the direction of integration of Weights & Biases API. 
            <br>
			<br>
			<img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/continualai/avalanche?style=social">
			&emsp;
			<img alt="GitHub forks" src="https://img.shields.io/github/forks/continualai/avalanche?style=social">
			&emsp;
			<a href="https://avalanche-api.continualai.org/" target="_blank"><img src="https://img.shields.io/badge/docs-passing?style=social&logo=Read the Docs"></a>
            <p></p>
            
          </td>
        </tr>

		<td width="25%" valign="center" align="center"><img src="images/echo.png" alt="nthu" width="160" height="170"></td>
          <td style="padding:20px;width:75%;vertical-align:middle">
            <a class="tog" href="https://github.com/digantamisra98/Echo" target="_blank">
              <papertitle>Echo</papertitle>
            </a>
            <br>
            Jun'19 - Present
            <br>
            <br>
			Echo is an OSS deep learning package with support for TensorFlow, PyTorch and MegEngine, containing novel validated methods, components and building blocks used in deep learning.
            <br>
			<br>
			<img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/digantamisra98/echo?style=social">
			&emsp;
			<img alt="GitHub forks" src="https://img.shields.io/github/forks/digantamisra98/echo?style=social">
			&emsp;
			<a href="https://pypi.python.org/pypi/echoAI" target="_blank"><img src="https://img.shields.io/pypi/dm/echoAI.svg?style=social&logo=PyPI"></a>
			&emsp;
			<a href="https://xa9ax.gitbook.io/echo/" target="_blank"><img src="https://img.shields.io/badge/docs-passing?style=social&logo=Read the Docs"></a>
            <p></p>
            
          </td>
        </tr>

		<td width="25%" valign="center" align="center"><img src="images/evonorm.png" alt="nthu" width="250" height="120"></td>
		<td style="padding:20px;width:75%;vertical-align:middle">
			<a class="tog" href="https://github.com/digantamisra98/EvoNorm" target="_blank">
			<papertitle>Evonorm</papertitle>
			</a>
			<br>
			Apr'20
			<br>
			<br>
			Created the most popular open source reimplementation of <a href="https://arxiv.org/abs/2004.02967" target="_blank">Evolving Normalization-Activation Layers</a> by Liu. et. al.
			<br>
			<br>
			<img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/digantamisra98/evonorm?style=social">
			&emsp;
			<img alt="GitHub forks" src="https://img.shields.io/github/forks/digantamisra98/evonorm?style=social">
			<p></p>
		</td>
		</tr>

		<td width="25%" valign="center" align="center"><img src="https://blog.paperspace.com/content/images/size/w1600/2020/09/eca_module.jpg" alt="eca" width="250" height="120"></td>
		<td style="padding:20px;width:75%;vertical-align:middle">
			<a class="tog" href="https://github.com/digantamisra98/Reproducibilty-Challenge-ECANET" target="_blank">
			<papertitle>ECANets</papertitle>
			</a>
			<br>
			Jan'21
			<br>
			<br>
			Reproduced the CVPR 2020 paper: <a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Wang_ECA-Net_Efficient_Channel_Attention_for_Deep_Convolutional_Neural_Networks_CVPR_2020_paper.html">ECA-Net: Efficient Channel Attention for Deep Convolutional Neural Networks</a> for the ML Reproducibility Challenge 2020. Integrated with <a href="https://wandb.ai">Weights & Biases</a>.
			<br>
			<br>
			<img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/digantamisra98/Reproducibilty-Challenge-ECANET?style=social">
			&emsp;
			<!---<img alt="GitHub forks" src="https://img.shields.io/github/forks/digantamisra98/Reproducibilty-Challenge-ECANET?style=social">
			&emsp;--->
			<a href="https://blog.paperspace.com/attention-mechanisms-in-computer-vision-ecanet/" target="_blank"><img src="https://img.shields.io/badge/Blog-Paperspace?style=social&logo=data:image/png;base64,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"></a>
			&emsp;
			<a href="https://wandb.ai/diganta/ECANet-sweep/reports/ECA-Net-Efficient-Channel-Attention-for-Deep-Convolutional-Neural-Networks--VmlldzozODU0NTM" target="_blank"><img src="https://img.shields.io/badge/Report-WandB?style=flat&color=black&logo=data:image/png;base64,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"></a>
			&emsp;
			<a href="https://mybinder.org/v2/gh/digantamisra98/Reproducibilty-Challenge-ECANET/HEAD" target="_blank"><img src="https://mybinder.org/badge_logo.svg"></a>
			<p></p>
		</td>
		</tr>

		<td width="25%" valign="center" align="center"><img src="images/BIG-bench.png" alt="nthu" width="120" height="120"></td>
		<td style="padding:20px;width:75%;vertical-align:middle">
			<a class="tog" href="https://github.com/google/BIG-bench" id="Big-Bench" target="_blank">
			<papertitle>Big Bench</papertitle>
			</a>
			<br>
			Aug'21
			<br>
			<br>
			Our <a href="https://github.com/google/BIG-bench/tree/main/bigbench/benchmark_tasks/tense" target="_blank">fine grained tense modification task</a> was accepted to <a href="https://github.com/google/BIG-bench" target="_blank">Google's Big Bench</a> for testing large LMs. In collaboration with <a href="https://in.linkedin.com/in/mukundvarmat" target="_blank">Mukund Varma T.</a>
			<br>
			<br>
			<img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/google/big-bench?style=social">
			&emsp;
			<img alt="GitHub forks" src="https://img.shields.io/github/forks/google/big-bench?style=social">
			<p></p>

		</td>
		</tr>


        </tbody></table>

		<br>
		<br>


		<table width="100%" border="0" cellspacing="15" cellpadding="10">
		<a id="education"><heading>&nbsp;&nbsp;&nbsp;Education</heading></a>

		<tr>
			<td width="15%" valign="center" align="center"><img src="images/mila.png" alt="nthu" width="112" height="130"></td>
			<td width="85%" valign="top">
			<p>
				<span><strong><a href="https://mila.quebec/en/person/diganta-misra/">Masters in Machine Learning</a></strong></span><span style="float:right">September 2021 - Present</span> <br>
				<em><a href="https://mila.quebec/" target="_blank">Montr√©al Institute of Learning Algorithms (MILA)</a></em>
				<br>
				Advisor: <a href="https://sites.google.com/site/irinarish/" target="_blank">Associate Professor Irina Rish</a>
				<br>
				Montr√©al, Canada
			</p>
			</td>
		</tr>

		<tr>
			<td width="15%" valign="center" align="center"><img src="images/udem.jpeg" alt="nthu" width="80" height="80"></td>
			<td width="85%" valign="top">
			<p>
				<span><strong>Masters of Science in Computer Science (MSc CS)</strong></span><span style="float:right">September 2021 - Present</span> <br>
				<em><a href="https://diro.umontreal.ca/accueil/" target="_blank">University of Montr√©al</a></em>
				<br>
				Advisor: <a href="https://sites.google.com/site/irinarish/" target="_blank">Associate Professor Irina Rish</a>
				<br>
				Montr√©al, Canada
				<br>
			</p>
			</td>
		</tr>
		
		<tr>
			<td width="15%" valign="center" align="center"><img src="images/kiit.jpg" alt="nthu" width="130" height="100"></td>
			<td width="85%" valign="top">
			<p>
				<span><strong>Bachelors of Technology (B.Tech) in EEE</strong></span><span style="float:right">Jun. 2016 - May. 2020</span> <br>
				<em><a href="https://kiit.ac.in/" target="_blank">Kalinga Institute of Industrial Technology (KIIT)</a></em>
				<br>
				Advisor: <a href="https://scholar.google.com/citations?user=iGAvOmEAAAAJ&hl=en" target="_blank">Asst. Prof. Dr. Bhargav Appasani</a>
				<br>
				Bhubaneswar, India
			</p>
			</td>
		</tr>
		</table>


		<table width="100%" border="0" cellspacing="15" cellpadding="10">
			<heading>&nbsp;&nbsp;&nbsp;Internships and Exchange Programs</heading>
	
			<tr>
				<td width="15%" valign="center" align="center"><img src="images/csir.png" alt="nthu" width="120" height="120"></td>
				<td width="85%" valign="top">
				<p>
					<span><strong>Data Science Intern</strong></span><span style="float:right">Jun. 2018 - Feb. 2019</span> <br>
					<em><a href="https://cdri.res.in/" target="_blank">CSIR-CDRI</a></em>
					<br>
					<br>
					During this internship, I was involved in building the analytical pipeline, data collection, pre-processing of data, cleaning of data, Geo-spatial Analysis of data and Document writing for the project on understanding demographics of Venture Capital and
					Early Seed Investments. As a part of a team of three, I was advised and mentored by <a href="http://brainnart.com/" target="_blank">Dr. Sukant Khurana</a>.
					<br>
					<br>
					Remote
				</p>
				</td>
			</tr>
	
			<tr>
				<td width="15%" valign="center" align="center"><img src="images/kgp.png" alt="nthu" width="120" height="120"></td>
				<td width="85%" valign="top">
				<p>
					<span><strong>Summer Intern</strong></span><span style="float:right">May. 2018 - Jun. 2018</span> <br>
					<em><a href="http://www.iitkgp.ac.in/" target="_blank">IIT-Kharagpur</a></em>
					<br>
					<br>
					Studied basic algorithmic techniques using functional programming languages - Lisp
					and Prolog under the guidance of <a href="http://www.iitkgp.ac.in/department/MA/faculty/ma-pawan#resp-tab1" target="_blank">Assc. Prof. Pawan Kumar</a>.
					<br>
					<br>
					Kharagpur, India
				</p>
				</td>
			</tr>
	
			<tr>
				<td width="15%" valign="center" align="center"><img src="images/bangkok.png" alt="nthu" width="120" height="120"></td>
				<td width="85%" valign="top">
				<p>
					<span><strong>Summer Exchange Intern</strong></span><span style="float:right">Jun. 2017 - Aug. 2017</span> <br>
					<em><a href="https://www.bu.ac.th/en/international-programs" target="_blank">Bangkok University</a></em>
					<br>
					<br>
					Served as a primary instructor for cultural engagements along with teaching basic
					english and computer science to primary grade students at RangsonWittaya School,
					Nakhon Sawan under the <a href="https://aiesec.org/" target="_blank">AIESEC</a> SDG #4 programme. Was also part of culture
					exchange, entrepreneurship and social service programs at Bangkok University
					<br>
					<br>
					Bangkok, Thailand
				</p>
				</td>
			</tr>
			</table>


        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Initiatives and Academic Services</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:20px;width:25%" valign="center" align="center"><img src="images/nma1.jpeg" height="80" width="160"></td>
            <td width="75%" valign="center">
                <a href="https://academy.neuromatch.io/" target="_blank">
                    <papertitle>NeuroMatch Academy</papertitle>
                </a>
                <br>
                <br>
				I am responsible for developing the content for the Strategies section in the Continual Learning lecture of the Deep Learning Cohort of Neuromatch Academy 2021.
              
            </td>
          </tr>
          <tr>
              <td style="padding:20px;width:25%" valign="center" align="center"><img src="images/wandb.jpeg" height="120" width="120"></td>
              <td width="75%" valign="center">
                  <a href="https://wandb.ai/site/reproducibility-challenge" target="_blank">
                      <papertitle>W&B ML Reproducibility Challenge</papertitle>
                  </a>
                  <br>
                  <br>
                  I am the lead organizer of the W&B MLRC 2021 where I actively support our challenge participants. Our mission of organizing this challenge is 
																		to make machine learning research reproducible, transparent and accessible to everyone. This initiative is also supported by our W&B MLRC Grant of $500 for each participant.
                
              </td>
            </tr>
			

			<tr>
			<td style="padding:20px;width:25%" valign="center" align="center"><img src="https://upload.wikimedia.org/wikipedia/en/thumb/4/4f/%C3%89cole_Polytechnique_de_Montr%C3%A9al_Logo.svg/1200px-%C3%89cole_Polytechnique_de_Montr%C3%A9al_Logo.svg.png" height="140" width="150"></td>
			<td width="75%" valign="center">
				<a href="https://www.polymtl.ca/programmes/cours/iatech-probabilistes-et-dapprentissage" target="_blank">
					<papertitle>INF8225: Probabilistic Learning</papertitle>
				</a>
				<br>
				<br>
				I am a teaching assistant for the <a href="https://www.polymtl.ca/programmes/cours/iatech-probabilistes-et-dapprentissage" target="_blank">INF8225: Probabilistic Learning</a> at Polytechnique University taught by <a href="https://mila.quebec/en/person/pal-christopher/" target="_blank">Christopher J. Pal</a> for the Winter 2022 semester.

			</td>
			</tr>

			  <tr>
					<td style="padding:20px;width:25%" valign="center" align="center"><img src="images/mila.png" height="150" width="132"></td>
					<td width="75%" valign="center">
						<a href="https://mila-dl-theory.github.io/" target="_blank">
							<papertitle>Deep Learning Theory Reading Group, MILA</papertitle>
						</a>
						<br>
						<br>
						I am an organizer of the DL Theory Reading Group at MILA, Montreal.
					</td>
					</tr>

				<tr>
					<td style="padding:20px;width:25%" valign="center" align="center"><img src="images/mila.png" height="150" width="132"></td>
					<td width="75%" valign="center">
						<papertitle>MILA 2022 Entrepreneurs Cohort Program</papertitle>
						<br>
						<br>
						I was selected as one of the entrepreneurs in residence and pitched my startup idea called <textbf>9<span style="color:rgb(255, 221, 0)">CLEF</span></textbf> (<a href="https://pitch.com/public/b150fe1f-47f0-439f-9ee4-58aad07b8d57" target="_blank">Elevator Pitch</a>).
					</td>
					</tr>

					<tr>
						<td style="padding:20px;width:25%" valign="center" align="center"><img src="https://pbs.twimg.com/media/FLkm2N-XIAQKQqC.jpg" height="120" width="400"></td>
						<td width="75%" valign="center">
								<papertitle>Served as a Program Committee member for:</papertitle>
							<br>
							<br>
							<a href="https://lifelong-ml.cc/" target="_blank">Conference on Lifelong Learning Agents(CoLLA) 2022</a>
						  
						</td>
					  </tr>
			
			
					<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
					  <tr>
						<td>
							<a id="achievements"><heading>Achievements</heading></a>
						  <br />(Complete list available upon request)
						</td>
					  </tr>
					</tbody></table>
					<table width="100%" align="center" border="0" cellpadding="20"><tbody>
					  <tr>
						<td style="padding:20px;width:25%" valign="center" align="center"><img src="https://upload.wikimedia.org/wikipedia/fr/thumb/6/6e/%C3%89ducation_et_Enseignement_sup%C3%A9rieur_Qu%C3%A9bec.svg/2560px-%C3%89ducation_et_Enseignement_sup%C3%A9rieur_Qu%C3%A9bec.svg.png" height="100" width="240"></td>
						<td width="75%" valign="center">
								<papertitle>Quebec Ministry of Higher Education International Students Scholarship <br /> 2022</papertitle>
							<br>
							<br>
							I was awarded the DIRO x Quebec Ministry of Higher Education international students scholarship worth CAD$4,000 for the academic year 2022.
						  
						</td>
					  </tr>
					  <tr>
						  <td style="padding:20px;width:25%" valign="center" align="center"><img src="https://pbs.twimg.com/profile_images/1195380768819892229/hkmnqsWf_400x400.jpg" height="120" width="120"></td>
						  <td width="75%" valign="center">
							  <a href="https://www.unique.quebec/2022-unique-excellence-scholarships" target="_blank">
								  <papertitle>UNIQUE AI Excellence Scholarship <br /> 2022</papertitle>
							  </a>
							  <br>
							  <br>
							  I was awarded the UNIQUE AI Excellence Scholarship worth CAD$10,000 for the academic year 2022. Under this scholarship, I will be working with <a href="https://sites.google.com/site/irinarish/" target="_blank">Irina Rish</a> and <a href="https://www.mcgill.ca/physiology/directory/core-faculty/pouya-bashivan" target="_blank">Pouya Bashivan</a> on dynamic sparsity based research.
							
						  </td>
						</tr>

						<tr>
							<td style="padding:20px;width:25%" valign="center" align="center"><img src="https://pbs.twimg.com/media/ELsiiN0WsAAkUYN.png" height="140" width="300"></td>
							<td width="75%" valign="center">
								<a href="https://paperswithcode.com/" target="_blank">
									<papertitle>PaperswithCode Top Contributor award <br /> 2022</papertitle>
								</a>
								<br>
								<br>
								I was awarded the PaperswithCode Top Contributor award for the academic year 2022.
							  
							</td>
						  </tr>

						<tr>
							<td style="padding:20px;width:25%" valign="center" align="center"><img src="images/mila.png" height="150" width="132"></td>
							<td width="75%" valign="center">
								<papertitle>MILA Entrepreneurs Grant<br /> 2022</papertitle>
								<br>
								<br>
								I was awarded the MILA Entrepreneurs Grant worth CAD$5,000 to pursue my startup venture <textbf>9<span style="color:rgb(255, 221, 0)">CLEF</span></textbf> (<a href="https://pitch.com/public/b150fe1f-47f0-439f-9ee4-58aad07b8d57" target="_blank">Elevator Pitch</a>) and build an early prototype.
							  
							</td>
						  </tr>

						<tr>
							<td style="padding:20px;width:25%" valign="center" align="center"><img src="https://images.squarespace-cdn.com/content/v1/619d40013a91687ef63e62d7/7e0aa3da-6cc6-4413-b279-2e9e4a88cab1/Amii-AI-Week-logo.png?format=1500w" height="60" width="240"></td>
							<td width="75%" valign="center">
								<a href="https://www.ai-week.ca/?utm_source=google-ads&utm_medium=cpc&utm_campaign=ai-week&utm_term=amii%20ai%20week&utm_campaign=AI-Week+%7C+S+%7C+Brand&utm_source=adwords&utm_medium=ppc&hsa_acc=6591753441&hsa_cam=16953749208&hsa_grp=135907011819&hsa_ad=593686735388&hsa_src=g&hsa_tgt=kwd-1650174358069&hsa_kw=amii%20ai%20week&hsa_mt=p&hsa_net=adwords&hsa_ver=3&gclid=CjwKCAjwve2TBhByEiwAaktM1BjAxiVdVUehV3fuuvfAgtH1vgzVT_jb-fmmTT6sbtfQSoxJ1RTJihoCLykQAvD_BwE" target="_blank">
									<papertitle>AMII AI Week Travel Bursary<br /> 2022</papertitle>
								</a>
								<br>
								<br>
								I was awarded the AMII AI Week 2022 Student Travel Bursary worth CAD$1,500.
							</td>
						  </tr>
			
						
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:left;font-size:small;">
                  Updated on: 22nd November, 2022
    
              <span style="float:right;">
                Merci, <a href="https://jonbarron.info/" target="_blank">Jon Barron</a>!
              </span>
                
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
</body>

</html>
